{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 7.12.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py:1416: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  self.tb = tb\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e0efe90e4ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IPython'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/terminal/embed.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m         frame.f_code.co_filename, frame.f_lineno), **kwargs)\n\u001b[1;32m    388\u001b[0m     shell(header=header, stack_depth=2, compile_flags=compile_flags,\n\u001b[0;32m--> 389\u001b[0;31m         _call_location_id='%s:%s' % (frame.f_code.co_filename, frame.f_lineno))\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0mInteractiveShellEmbed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;31m#restore previous instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/terminal/embed.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, header, local_ns, module, dummy, stack_depth, global_ns, compile_flags, **kw)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# our call and get the original caller's namespaces.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         self.mainloop(local_ns, module, stack_depth=stack_depth,\n\u001b[0;32m--> 229\u001b[0;31m                       global_ns=global_ns, compile_flags=compile_flags)\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbanner2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_banner2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/terminal/embed.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, local_ns, module, stack_depth, display_banner, global_ns, compile_flags)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# now, purge out the local namespace of IPython's hidden variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py\u001b[0m in \u001b[0;36minteract\u001b[0;34m(self, display_banner)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_for_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfirm_exit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/terminal/interactiveshell.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mprompt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_prompt_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0mprompt_continuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuation_prompt_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'incomplete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/utils/py3compat.py\u001b[0m in \u001b[0;36minput\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# to forward requests to a frontend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuiltin_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mbuiltin_mod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"builtins\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import requests\n",
    "\n",
    "import gzip\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sqlite3 as sql\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy import Boolean, Column, Float, String, Integer\n",
    "import uvicorn\n",
    "\n",
    "from flask import Flask, flash, redirect, render_template, request, url_for\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "__import__('IPython').embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py:187: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py:187: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 21.2 s, total: 2min 32s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url_list = ['https://datasets.imdbws.com/title.akas.tsv.gz', 'https://datasets.imdbws.com/title.basics.tsv.gz', 'https://datasets.imdbws.com/title.crew.tsv.gz','https://datasets.imdbws.com/title.principals.tsv.gz','https://datasets.imdbws.com/title.ratings.tsv.gz', 'https://datasets.imdbws.com/name.basics.tsv.gz']\n",
    "\n",
    "def read_url(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    with open(filename, \"wb\") as f:\n",
    "        r = requests.get(url)\n",
    "        f.write(r.content)                  #save gz file\n",
    "    with gzip.open(filename, 'rb') as f:    #open gz file and store into dataframe\n",
    "        df = pd.read_csv(f,sep=\"\\t\")  \n",
    "    return df\n",
    "\n",
    "title_df = read_url(url_list[0])\n",
    "title_df2 = read_url(url_list[1])\n",
    "crew_df = read_url(url_list[2])\n",
    "cast_df = read_url(url_list[3])\n",
    "ratings_df = read_url(url_list[4])\n",
    "names_df = read_url(url_list[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since reading data takes only around 4 minutes and we only need to run this once in a while, it should be fine without optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def process_duration(duration):\n",
    "    try:\n",
    "        return int(duration)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def process_title(title):\n",
    "    if title == 'tvMovie':\n",
    "        return 'movie'\n",
    "    elif title == 'tvEpisode' or title == 'tvMiniSeries':\n",
    "        return 'tvSeries'\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "def process_string(string):           #split string into list, keep first 3 actors\n",
    "    actor_list = string.split(',')\n",
    "    if len(actor_list) > 3:\n",
    "        actor_list = actor_list[:3]\n",
    "    return actor_list\n",
    "\n",
    "title_df_region = title_df.loc[title_df.region.isin(['US', 'GB', 'TW', 'CN', 'HK', 'KR'])]\n",
    "title_df_region = title_df_region.drop_duplicates(subset=['titleId'])\n",
    "title_df2['startYear'] = title_df2['startYear'].apply(process_duration)\n",
    "title_df2['runtimeMinutes'] = title_df2['runtimeMinutes'].apply(process_duration)\n",
    "title_df2_startYear = title_df2.loc[title_df2['startYear'] >= 2000]\n",
    "combined_df = title_df_region.merge(title_df2_startYear, left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1).merge(ratings_df, left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1)\n",
    "combined_df2 = combined_df.loc[combined_df['titleType'].isin(['movie','tvMovie','tvEpisode','tvSeries','tvMiniSeries'])]\n",
    "combined_df2['titleType'] = combined_df2['titleType'].apply(process_title)\n",
    "crew_df_combined = crew_df.merge(names_df, left_on = 'directors', right_on = 'nconst').drop(['nconst'],axis = 1)\n",
    "directors = crew_df_combined[['tconst', 'directors', 'primaryName']]\n",
    "directors2 = directors.loc[directors.tconst.isin(combined_df2.titleId)==True] \n",
    "cast_df_combined = cast_df.loc[cast_df.category.isin(['actor','actress'])].merge(names_df, left_on = 'nconst', right_on = 'nconst')\n",
    "actors = cast_df_combined[['tconst', 'nconst', 'primaryName']]\n",
    "actors2 = actors.loc[actors.tconst.isin(combined_df2.titleId)==True]    #filter by selected movies\n",
    "actors_string_nconst = actors2.groupby('tconst')['nconst'].apply(lambda x: ','.join(x)).reset_index()\n",
    "actors_string_name = actors2.groupby('tconst')['primaryName'].apply(lambda x: ','.join(x)).reset_index()\n",
    "actors_string_nconst['nconst'] = actors_string_nconst['nconst'].apply(process_string)\n",
    "actors_string_name['primaryName'] = actors_string_name['primaryName'].apply(process_string)\n",
    "actors_df = actors_string_nconst.merge(actors_string_name, on = 'tconst')\n",
    "final_df = combined_df2.merge(actors_df, how = 'left', left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1).rename(columns = {'nconst': 'actor_id', 'primaryName': 'actor_name'}, inplace = False)\n",
    "final_df2 = final_df.merge(directors2, how = 'left', left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1).rename(columns = {'directors': 'director_id', 'primaryName': 'director_name', 'titleId': 'title_id', 'titleType': 'movie_type'}, inplace = False)\n",
    "final_df3 = final_df2[['title_id', 'title', 'region', 'movie_type', 'genres', 'actor_id', 'actor_name', 'director_id', 'director_name', 'startYear', 'runtimeMinutes', 'averageRating', 'numVotes']]\n",
    "final_df4 = final_df3.query('averageRating >= 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 16.5 ms, total: 27.7 ms\n",
      "Wall time: 41.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_df4_sim = final_df4.iloc[0:1000].copy()\n",
    "\n",
    "# Function to convert all strings to lower case and strip names of spaces, as well as list\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "def clean_genres(genre_str):        #for loop required, since need to replace 'Action' and 'Adventure' at the same time\n",
    "    if genre_str.lower() == '\\n' or genre_str.lower() == '\\\\n':\n",
    "        genre_str = \"\"\n",
    "    genre_str = genre_str.replace(\"Short\", \"\")\n",
    "    genre_str = genre_str.replace(\"Western\", \"\")\n",
    "    genre_str = genre_str.replace(\"Adult\", \"\")\n",
    "    genre_str = genre_str.replace(\"War\", \"\")\n",
    "    \n",
    "    if genre_str != \"\":\n",
    "        genre_list = []\n",
    "        genre_str = genre_str.split(',')\n",
    "        for genre in genre_str:\n",
    "            if genre == 'Action' or genre == 'Adventure':\n",
    "                genre_list.append('Action & Adventure')\n",
    "            elif genre == 'Sci-Fi' or genre == 'Fantasy':\n",
    "                genre_list.append('Sci-Fi & Fantasy')\n",
    "            elif genre == 'Musical':\n",
    "                genre_list.append('Music')\n",
    "            elif genre == 'Biography' or genre == 'History' or genre == 'News':\n",
    "                genre_list.append('Documentary')\n",
    "            elif genre == 'Game-Show' or genre == 'Talk-Show' or genre == 'Reality-TV':\n",
    "                genre_list.append('Reality TV and Talk shows')\n",
    "            elif genre == '':\n",
    "                pass\n",
    "            else:\n",
    "                genre_list.append(genre)\n",
    "        genre_list = list(set(genre_list))\n",
    "    else:\n",
    "        genre_list = genre_str\n",
    "    return genre_list\n",
    "\n",
    "final_df4_sim['actor_name_clean'] = final_df4_sim['actor_name'].apply(clean_data)\n",
    "final_df4_sim['director_name_clean'] = final_df4_sim['director_name'].apply(clean_data)\n",
    "final_df4_sim['genres_list'] = final_df4_sim['genres'].apply(clean_genres)\n",
    "final_df4_sim['genres_list_lower'] = final_df4_sim['genres_list'].apply(lambda x: [genre.lower() for genre in x])\n",
    "final_df4_sim = final_df4_sim.loc[final_df4_sim.genres_list != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping overview, image and keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we requested from IMDB website doesn't have overview, image and keywords. We need the overview and image as they are important information we want to show users in our recommendations. Keywords may be important features for determining movie similarity as well.\n",
    "\n",
    "Let's try several ways to scrape the data:\n",
    "\n",
    "1) Using .apply and separate functions\n",
    "   - This is very inefficient, since we're making requests multiple times to the same url\n",
    "   - This took 8 mins 20 sec for 100 links.\n",
    "   \n",
    "2) Using loop over numpy array and one function\n",
    "   - This is more efficient, since we're making request 1 time to the url to extract info\n",
    "   - This took 6 mins for 100 links. But we still need to store the info in columns\n",
    "   \n",
    "3) Using Async IO\n",
    "   - Using asychronous process instead of sequential to get speedup\n",
    "   - This works very well and only took 30s for 100 links. But we still need to store the info in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.1 s, sys: 1.81 s, total: 37.9 s\n",
      "Wall time: 8min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_overview(row):\n",
    "    try:\n",
    "        url = \"https://www.imdb.com/title/%s/\" %(row.title_id) \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tag = soup.find('div', attrs={'class': 'ipc-html-content ipc-html-content--base'})         #seems to only work for unique classes\n",
    "        return tag.getText()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_image(row):\n",
    "    try:\n",
    "        title_name = row.title\n",
    "        actors = row.actor_name_clean\n",
    "        url = \"https://www.imdb.com/title/%s/\" %(row.title_id)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        images = soup.findAll('img') \n",
    "        for img in images:\n",
    "            if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']) or actors[0] in (img['alt']):    #actor_list should have at least 1 element\n",
    "                image_final = img['src']\n",
    "                return image_final\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "def get_keywords(row):\n",
    "    try:\n",
    "        url = \"https://www.imdb.com/title/%s/keywords\" %(row.title_id)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tag = soup.findAll('div', attrs={'class': 'sodatext'})\n",
    "        keyword_list = []\n",
    "        for keyword in tag[0:5]:\n",
    "            keyword_list.append(keyword.getText().strip())\n",
    "        return keyword_list\n",
    "    except:\n",
    "        return \"\"\n",
    " \n",
    "final_df4_sim['overview'] = final_df4_sim.apply(get_overview, axis = 1)\n",
    "final_df4_sim['image'] = final_df4_sim.apply(get_image, axis = 1)\n",
    "final_df4_sim['keywords'] = final_df4_sim.apply(get_keywords, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 994 ms, total: 23.2 s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def scrape_info(df_arr):\n",
    "        title_id = df_arr[0]\n",
    "        title_name = df_arr[1]\n",
    "        actors = df_arr[6]\n",
    "\n",
    "        url = \"https://www.imdb.com/title/%s/\" %(title_id) \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        #Get overview\n",
    "        tag = soup.find('div', attrs={'class': 'ipc-html-content ipc-html-content--base'})         #seems to only work for unique classes\n",
    "        if tag != None:\n",
    "            overview = tag.getText()\n",
    "        else:\n",
    "            overview = float('nan')\n",
    "            \n",
    "        #Get images\n",
    "        images = soup.findAll('img') \n",
    "        image_final = float('nan')\n",
    "        for img in images:\n",
    "            if isinstance(actors,list):\n",
    "                if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']) or actors[0] in (img['alt']):    #actor_list should have at least 1 element\n",
    "                    image_final = img['src']\n",
    "            else:\n",
    "                if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']):    #actor_list should have at least 1 element\n",
    "                    image_final = img['src']\n",
    "\n",
    "        #Get keywords\n",
    "        url = \"https://www.imdb.com/title/%s/keywords\" %(title_id)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tag = soup.findAll('div', attrs={'class': 'sodatext'})\n",
    "        keyword_list = []\n",
    "        for keyword in tag[0:5]:      \n",
    "            keyword_list.append(keyword.getText().strip())\n",
    "        \n",
    "        return (overview, image_final, keyword_list)\n",
    "\n",
    "title_df = {}\n",
    "title_array = final_df4_sim.to_numpy()\n",
    "for title in title_array:\n",
    "    title_df[title[0]] = scrape_info(title)    #store dict with format {title_id: (overview, image, keyword_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 s, sys: 1.89 s, total: 20.8 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "async def scrape_info(session, df_arr):\n",
    "        title_id = df_arr[0]\n",
    "        title_name = df_arr[1]\n",
    "        actors = df_arr[6]\n",
    "        \n",
    "        \n",
    "        url = \"https://m.imdb.com/title/%s/\" %(title_id)\n",
    "        async with session.get(url) as response:\n",
    "            response_text = await response.text()       #need to use response.text() instead of response.text which is an object instead of function\n",
    "            soup = BeautifulSoup(response_text, 'lxml')\n",
    "\n",
    "            #Get overview\n",
    "            tag = soup.find('div', attrs={'class': 'ipc-html-content ipc-html-content--base'})         #seems to only work for unique classes\n",
    "            if tag != None:\n",
    "                overview = tag.getText()\n",
    "            else:\n",
    "                overview = float('nan')\n",
    "\n",
    "            #Get images\n",
    "            images = soup.findAll('img') \n",
    "            image_final = float('nan')\n",
    "            for img in images:\n",
    "                if isinstance(actors,list):\n",
    "                    if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']) or actors[0] in (img['alt']):    #actor_list should have at least 1 element\n",
    "                        image_final = img['src']\n",
    "                else:\n",
    "                    if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']):    #actor_list should have at least 1 element\n",
    "                        image_final = img['src']\n",
    "\n",
    "        #Get keywords\n",
    "        url_kw = \"https://m.imdb.com/title/%s/keywords\" %(title_id)\n",
    "        async with session.get(url_kw) as response_kw:\n",
    "            response_kw_text = await response_kw.text()\n",
    "            soup_kw = BeautifulSoup(response_kw_text, 'lxml')\n",
    "            tag_kw = soup_kw.findAll('div', attrs={'class': 'sodatext'})\n",
    "            keyword_list = []\n",
    "            for keyword in tag_kw[0:5]:      \n",
    "                keyword_list.append(keyword.getText().strip())\n",
    "        return (title_id, overview, image_final, keyword_list)\n",
    "    \n",
    "async def main(): \n",
    "    title_array = final_df4_sim.to_numpy()       \n",
    "    async with ClientSession() as session:\n",
    "        title_list = await asyncio.gather(*[scrape_info(session, title) for title in title_array])\n",
    "        return title_list\n",
    "    \n",
    "title_list = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df = pd.DataFrame(title_list, columns=['title_id', 'overview', 'image', 'keywords'])   #convert list of tuples to df\n",
    "final_df4_sim2 = final_df4_sim.merge(scraped_df, how = 'left', on = 'title_id') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarity and identifying most similar movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    return x['region'] + ' ' + x['movie_type'] + ' ' + ' '.join(x['actor_name_clean']) + ' ' + x['director_name_clean'] + ' ' + ' '.join(x['genres_list_lower']) + ' ' + ' '.join(x['keywords'])\n",
    "\n",
    "final_df4_sim2['soup'] = final_df4_sim2.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN movie xuepengfan yi-minwen shaoquanzhu yi-minwen action & adventure justice'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df4_sim2['soup'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5586)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(final_df4_sim2['soup'])\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since movies may not go in order \n",
    "final_df4_sim2 = final_df4_sim2.reset_index(drop = True)\n",
    "\n",
    "#Create Series where index is title_id, and column is index\n",
    "indices = pd.Series(final_df4_sim2.index, index=final_df4_sim2['title_id'])   \n",
    "\n",
    "def insert_top_movies(row):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[row.title_id]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))      #creates list of tuple (index, similarity score)\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 30 most similar movies\n",
    "    sim_scores = sim_scores[1:31]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    #Return top 10 movies with highest rating\n",
    "    top_movies = final_df4_sim2.iloc[movie_indices].sort_values(by = 'averageRating', ascending = False)[['title_id','title','image']][0:10] \n",
    "    top_movies_tuple = list(zip(top_movies.title_id, top_movies.title, top_movies.image))\n",
    "    return top_movies_tuple\n",
    "\n",
    "final_df4_sim2['top_movies'] = final_df4_sim2.apply(insert_top_movies, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into movie database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_columns = ['title_id', 'title', 'region', 'movie_type', 'genres', 'director_id', 'director_name', 'director_name_clean', 'overview', 'image', 'soup']\n",
    "list_columns = ['actor_id', 'actor_name', 'actor_name_clean', 'genres_list', 'genres_list_lower', 'keywords', 'top_movies']\n",
    "\n",
    "for column in str_columns:\n",
    "    final_df4_sim2[column] = final_df4_sim2[column].astype(str) \n",
    "\n",
    "for column in list_columns:\n",
    "    final_df4_sim2[column] = final_df4_sim2[column].apply(lambda x: json.dumps(x))     #SQLite3 doesn't accept array. Cannot use .to_json() on entire column, since will return same value for all rows\n",
    "\n",
    "movie_db = sql.connect('movie_recsys4.db')\n",
    "final_df4_sim2.to_sql('movie', movie_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the database containing the list of movies, essential information and most similar movies, we can create our Flask app to interact with the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
