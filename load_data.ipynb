{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import requests\n",
    "\n",
    "import gzip\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sqlite3 as sql\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy import Boolean, Column, Float, String, Integer\n",
    "import uvicorn\n",
    "\n",
    "from flask import Flask, flash, redirect, render_template, request, url_for\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# __import__('IPython').embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py:187: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py:187: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 21.2 s, total: 2min 32s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url_list = ['https://datasets.imdbws.com/title.akas.tsv.gz', 'https://datasets.imdbws.com/title.basics.tsv.gz', 'https://datasets.imdbws.com/title.crew.tsv.gz','https://datasets.imdbws.com/title.principals.tsv.gz','https://datasets.imdbws.com/title.ratings.tsv.gz', 'https://datasets.imdbws.com/name.basics.tsv.gz']\n",
    "\n",
    "def read_url(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    with open(filename, \"wb\") as f:\n",
    "        r = requests.get(url)\n",
    "        f.write(r.content)                  #save gz file\n",
    "    with gzip.open(filename, 'rb') as f:    #open gz file and store into dataframe\n",
    "        df = pd.read_csv(f,sep=\"\\t\")  \n",
    "    return df\n",
    "\n",
    "title_df = read_url(url_list[0])\n",
    "title_df2 = read_url(url_list[1])\n",
    "crew_df = read_url(url_list[2])\n",
    "cast_df = read_url(url_list[3])\n",
    "ratings_df = read_url(url_list[4])\n",
    "names_df = read_url(url_list[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since reading data takes only around 4 minutes and we only need to run this once in a while, it should be fine without optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def process_duration(duration):\n",
    "    try:\n",
    "        return int(duration)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def process_title(title):\n",
    "    if title == 'tvMovie':\n",
    "        return 'movie'\n",
    "    elif title == 'tvEpisode' or title == 'tvMiniSeries':\n",
    "        return 'tvSeries'\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "def process_string(string):           #split string into list, keep first 3 actors\n",
    "    actor_list = string.split(',')\n",
    "    if len(actor_list) > 3:\n",
    "        actor_list = actor_list[:3]\n",
    "    return actor_list\n",
    "\n",
    "title_df_region = title_df.loc[title_df.region.isin(['US', 'GB', 'TW', 'CN', 'HK', 'KR'])]\n",
    "title_df_region = title_df_region.drop_duplicates(subset=['titleId'])\n",
    "title_df2['startYear'] = title_df2['startYear'].apply(process_duration)\n",
    "title_df2['runtimeMinutes'] = title_df2['runtimeMinutes'].apply(process_duration)\n",
    "title_df2_startYear = title_df2.loc[title_df2['startYear'] >= 2000]\n",
    "combined_df = title_df_region.merge(title_df2_startYear, left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1).merge(ratings_df, left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1)\n",
    "combined_df2 = combined_df.loc[combined_df['titleType'].isin(['movie','tvMovie','tvEpisode','tvSeries','tvMiniSeries'])]\n",
    "combined_df2['titleType'] = combined_df2['titleType'].apply(process_title)\n",
    "crew_df_combined = crew_df.merge(names_df, left_on = 'directors', right_on = 'nconst').drop(['nconst'],axis = 1)\n",
    "directors = crew_df_combined[['tconst', 'directors', 'primaryName']]\n",
    "directors2 = directors.loc[directors.tconst.isin(combined_df2.titleId)==True] \n",
    "cast_df_combined = cast_df.loc[cast_df.category.isin(['actor','actress'])].merge(names_df, left_on = 'nconst', right_on = 'nconst')\n",
    "actors = cast_df_combined[['tconst', 'nconst', 'primaryName']]\n",
    "actors2 = actors.loc[actors.tconst.isin(combined_df2.titleId)==True]    #filter by selected movies\n",
    "actors_string_nconst = actors2.groupby('tconst')['nconst'].apply(lambda x: ','.join(x)).reset_index()\n",
    "actors_string_name = actors2.groupby('tconst')['primaryName'].apply(lambda x: ','.join(x)).reset_index()\n",
    "actors_string_nconst['nconst'] = actors_string_nconst['nconst'].apply(process_string)\n",
    "actors_string_name['primaryName'] = actors_string_name['primaryName'].apply(process_string)\n",
    "actors_df = actors_string_nconst.merge(actors_string_name, on = 'tconst')\n",
    "final_df = combined_df2.merge(actors_df, how = 'left', left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1).rename(columns = {'nconst': 'actor_id', 'primaryName': 'actor_name'}, inplace = False)\n",
    "final_df2 = final_df.merge(directors2, how = 'left', left_on = 'titleId', right_on = 'tconst').drop(['tconst'],axis=1).rename(columns = {'directors': 'director_id', 'primaryName': 'director_name', 'titleId': 'title_id', 'titleType': 'movie_type'}, inplace = False)\n",
    "final_df3 = final_df2[['title_id', 'title', 'region', 'movie_type', 'genres', 'actor_id', 'actor_name', 'director_id', 'director_name', 'startYear', 'runtimeMinutes', 'averageRating', 'numVotes']]\n",
    "final_df4 = final_df3.query('averageRating >= 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 16.5 ms, total: 27.7 ms\n",
      "Wall time: 41.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_df4_sim = final_df4.iloc[0:1000].copy()\n",
    "\n",
    "# Function to convert all strings to lower case and strip names of spaces, as well as list\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "def clean_genres(genre_str):        #for loop required, since need to replace 'Action' and 'Adventure' at the same time\n",
    "    if genre_str.lower() == '\\n' or genre_str.lower() == '\\\\n':\n",
    "        genre_str = \"\"\n",
    "    genre_str = genre_str.replace(\"Short\", \"\")\n",
    "    genre_str = genre_str.replace(\"Western\", \"\")\n",
    "    genre_str = genre_str.replace(\"Adult\", \"\")\n",
    "    genre_str = genre_str.replace(\"War\", \"\")\n",
    "    \n",
    "    if genre_str != \"\":\n",
    "        genre_list = []\n",
    "        genre_str = genre_str.split(',')\n",
    "        for genre in genre_str:\n",
    "            if genre == 'Action' or genre == 'Adventure':\n",
    "                genre_list.append('Action & Adventure')\n",
    "            elif genre == 'Sci-Fi' or genre == 'Fantasy':\n",
    "                genre_list.append('Sci-Fi & Fantasy')\n",
    "            elif genre == 'Musical':\n",
    "                genre_list.append('Music')\n",
    "            elif genre == 'Biography' or genre == 'History' or genre == 'News':\n",
    "                genre_list.append('Documentary')\n",
    "            elif genre == 'Game-Show' or genre == 'Talk-Show' or genre == 'Reality-TV':\n",
    "                genre_list.append('Reality TV and Talk shows')\n",
    "            elif genre == '':\n",
    "                pass\n",
    "            else:\n",
    "                genre_list.append(genre)\n",
    "        genre_list = list(set(genre_list))\n",
    "    else:\n",
    "        genre_list = genre_str\n",
    "    return genre_list\n",
    "\n",
    "final_df4_sim['actor_name_clean'] = final_df4_sim['actor_name'].apply(clean_data)\n",
    "final_df4_sim['director_name_clean'] = final_df4_sim['director_name'].apply(clean_data)\n",
    "final_df4_sim['genres_list'] = final_df4_sim['genres'].apply(clean_genres)\n",
    "final_df4_sim['genres_list_lower'] = final_df4_sim['genres_list'].apply(lambda x: [genre.lower() for genre in x])\n",
    "final_df4_sim = final_df4_sim.loc[final_df4_sim.genres_list != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping overview, image and keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we requested from IMDB website doesn't have overview, image and keywords. We need the overview and image as they are important information we want to show users in our recommendations. Keywords may be important features for determining movie similarity as well.\n",
    "\n",
    "Let's try several ways to scrape the data:\n",
    "\n",
    "1) Using .apply and separate functions\n",
    "   - This is very inefficient, since we're making requests multiple times to the same url\n",
    "   - This took 8 mins 20 sec for 100 links.\n",
    "   \n",
    "2) Using loop over numpy array and one function\n",
    "   - This is more efficient, since we're making request 1 time to the url to extract info\n",
    "   - This took 6 mins for 100 links. But we still need to store the info in columns\n",
    "   \n",
    "3) Using Async IO\n",
    "   - Using asychronous process instead of sequential to get speedup\n",
    "   - This works very well and only took 30s for 100 links. But we still need to store the info in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.1 s, sys: 1.81 s, total: 37.9 s\n",
      "Wall time: 8min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_overview(row):\n",
    "    try:\n",
    "        url = \"https://www.imdb.com/title/%s/\" %(row.title_id) \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tag = soup.find('div', attrs={'class': 'ipc-html-content ipc-html-content--base'})         #seems to only work for unique classes\n",
    "        return tag.getText()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_image(row):\n",
    "    try:\n",
    "        title_name = row.title\n",
    "        actors = row.actor_name_clean\n",
    "        url = \"https://www.imdb.com/title/%s/\" %(row.title_id)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        images = soup.findAll('img') \n",
    "        for img in images:\n",
    "            if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']) or actors[0] in (img['alt']):    #actor_list should have at least 1 element\n",
    "                image_final = img['src']\n",
    "                return image_final\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "def get_keywords(row):\n",
    "    try:\n",
    "        url = \"https://www.imdb.com/title/%s/keywords\" %(row.title_id)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tag = soup.findAll('div', attrs={'class': 'sodatext'})\n",
    "        keyword_list = []\n",
    "        for keyword in tag[0:5]:\n",
    "            keyword_list.append(keyword.getText().strip())\n",
    "        return keyword_list\n",
    "    except:\n",
    "        return \"\"\n",
    " \n",
    "final_df4_sim['overview'] = final_df4_sim.apply(get_overview, axis = 1)\n",
    "final_df4_sim['image'] = final_df4_sim.apply(get_image, axis = 1)\n",
    "final_df4_sim['keywords'] = final_df4_sim.apply(get_keywords, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 994 ms, total: 23.2 s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def scrape_info(df_arr):\n",
    "        title_id = df_arr[0]\n",
    "        title_name = df_arr[1]\n",
    "        actors = df_arr[6]\n",
    "\n",
    "        url = \"https://www.imdb.com/title/%s/\" %(title_id) \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        #Get overview\n",
    "        tag = soup.find('div', attrs={'class': 'ipc-html-content ipc-html-content--base'})         #seems to only work for unique classes\n",
    "        if tag != None:\n",
    "            overview = tag.getText()\n",
    "        else:\n",
    "            overview = float('nan')\n",
    "            \n",
    "        #Get images\n",
    "        images = soup.findAll('img') \n",
    "        image_final = float('nan')\n",
    "        for img in images:\n",
    "            if isinstance(actors,list):\n",
    "                if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']) or actors[0] in (img['alt']):    #actor_list should have at least 1 element\n",
    "                    image_final = img['src']\n",
    "            else:\n",
    "                if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']):    #actor_list should have at least 1 element\n",
    "                    image_final = img['src']\n",
    "\n",
    "        #Get keywords\n",
    "        url = \"https://www.imdb.com/title/%s/keywords\" %(title_id)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tag = soup.findAll('div', attrs={'class': 'sodatext'})\n",
    "        keyword_list = []\n",
    "        for keyword in tag[0:5]:      \n",
    "            keyword_list.append(keyword.getText().strip())\n",
    "        \n",
    "        return (overview, image_final, keyword_list)\n",
    "\n",
    "title_df = {}\n",
    "title_array = final_df4_sim.to_numpy()\n",
    "for title in title_array:\n",
    "    title_df[title[0]] = scrape_info(title)    #store dict with format {title_id: (overview, image, keyword_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 s, sys: 1.89 s, total: 20.8 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "async def scrape_info(session, df_arr):\n",
    "        title_id = df_arr[0]\n",
    "        title_name = df_arr[1]\n",
    "        actors = df_arr[6]\n",
    "        \n",
    "        \n",
    "        url = \"https://m.imdb.com/title/%s/\" %(title_id)\n",
    "        async with session.get(url) as response:\n",
    "            response_text = await response.text()       #need to use response.text() instead of response.text which is an object instead of function\n",
    "            soup = BeautifulSoup(response_text, 'lxml')\n",
    "\n",
    "            #Get overview\n",
    "            tag = soup.find('div', attrs={'class': 'ipc-html-content ipc-html-content--base'})         #seems to only work for unique classes\n",
    "            if tag != None:\n",
    "                overview = tag.getText()\n",
    "            else:\n",
    "                overview = float('nan')\n",
    "\n",
    "            #Get images\n",
    "            images = soup.findAll('img') \n",
    "            image_final = float('nan')\n",
    "            for img in images:\n",
    "                if isinstance(actors,list):\n",
    "                    if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']) or actors[0] in (img['alt']):    #actor_list should have at least 1 element\n",
    "                        image_final = img['src']\n",
    "                else:\n",
    "                    if 'Poster' in (img['alt']) or 'Trailer' in (img['alt']) or title_name in (img['alt']):    #actor_list should have at least 1 element\n",
    "                        image_final = img['src']\n",
    "\n",
    "        #Get keywords\n",
    "        url_kw = \"https://m.imdb.com/title/%s/keywords\" %(title_id)\n",
    "        async with session.get(url_kw) as response_kw:\n",
    "            response_kw_text = await response_kw.text()\n",
    "            soup_kw = BeautifulSoup(response_kw_text, 'lxml')\n",
    "            tag_kw = soup_kw.findAll('div', attrs={'class': 'sodatext'})\n",
    "            keyword_list = []\n",
    "            for keyword in tag_kw[0:5]:      \n",
    "                keyword_list.append(keyword.getText().strip())\n",
    "        return (title_id, overview, image_final, keyword_list)\n",
    "    \n",
    "async def main(): \n",
    "    title_array = final_df4_sim.to_numpy()       \n",
    "    async with ClientSession() as session:\n",
    "        title_list = await asyncio.gather(*[scrape_info(session, title) for title in title_array])\n",
    "        return title_list\n",
    "    \n",
    "title_list = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df = pd.DataFrame(title_list, columns=['title_id', 'overview', 'image', 'keywords'])   #convert list of tuples to df\n",
    "final_df4_sim2 = final_df4_sim.merge(scraped_df, how = 'left', on = 'title_id') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarity and identifying most similar movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    return x['region'] + ' ' + x['movie_type'] + ' ' + ' '.join(x['actor_name_clean']) + ' ' + x['director_name_clean'] + ' ' + ' '.join(x['genres_list_lower']) + ' ' + ' '.join(x['keywords'])\n",
    "\n",
    "final_df4_sim2['soup'] = final_df4_sim2.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN movie xuepengfan yi-minwen shaoquanzhu yi-minwen action & adventure justice'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df4_sim2['soup'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5586)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(final_df4_sim2['soup'])\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since movies may not go in order \n",
    "final_df4_sim2 = final_df4_sim2.reset_index(drop = True)\n",
    "\n",
    "#Create Series where index is title_id, and column is index\n",
    "indices = pd.Series(final_df4_sim2.index, index=final_df4_sim2['title_id'])   \n",
    "\n",
    "def insert_top_movies(row):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[row.title_id]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))      #creates list of tuple (index, similarity score)\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 30 most similar movies\n",
    "    sim_scores = sim_scores[1:31]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    #Return top 10 movies with highest rating\n",
    "    top_movies = final_df4_sim2.iloc[movie_indices].sort_values(by = 'averageRating', ascending = False)[['title_id','title','image']][0:10] \n",
    "    top_movies_tuple = list(zip(top_movies.title_id, top_movies.title, top_movies.image))\n",
    "    return top_movies_tuple\n",
    "\n",
    "final_df4_sim2['top_movies'] = final_df4_sim2.apply(insert_top_movies, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into movie database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_columns = ['title_id', 'title', 'region', 'movie_type', 'genres', 'director_id', 'director_name', 'director_name_clean', 'overview', 'image', 'soup']\n",
    "list_columns = ['actor_id', 'actor_name', 'actor_name_clean', 'genres_list', 'genres_list_lower', 'keywords', 'top_movies']\n",
    "\n",
    "for column in str_columns:\n",
    "    final_df4_sim2[column] = final_df4_sim2[column].astype(str) \n",
    "\n",
    "for column in list_columns:\n",
    "    final_df4_sim2[column] = final_df4_sim2[column].apply(lambda x: json.dumps(x))     #SQLite3 doesn't accept array. Cannot use .to_json() on entire column, since will return same value for all rows\n",
    "\n",
    "movie_db = sql.connect('movie_recsys4.db')\n",
    "final_df4_sim2.to_sql('movie', movie_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the database containing the list of movies, essential information and most similar movies, we can create our Flask app to interact with the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
